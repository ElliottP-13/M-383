\documentclass[11pt]{article}
\usepackage{../EllioStyle}

\title{Homework 10}
\author{Elliott Pryor}
\date{28 Oct 2020}
\rhead{Elliott Pryor}


\renewcommand{\sp}{\; \;}

\begin{document}
\maketitle

\problem{1} 5.3.4 Problem 1

Define:
$$x_+ = \begin{cases}
x & \sp if \sp x \geq 0,\\
0 & \sp if \sp x < 0
\end{cases}$$

Prove that $f(x) = x^k_+$ is continuously differentiable if $k \in \integers, \sp k > 1$
\hrule

First we show that: 
$\frac{d}{dx} x^k = k \cdot x^{k-1}$

\begin{proof}
By induction:

We have the base case of $k = 2$. Then $f(x) = x^2 = x \cdot x$. We can use the product rule and theorem 5.3.1 to simplify this.
Let $g(x) = h(x) = x$ so $f(x) = (g \cdot h)(x)$. Then $f' = f(x)g'(x) + f'(x)g(x) = (x)(1) + (1)(x) = 2x$.
So the base case holds. 

Now we assume that for $f(x) = x^m$ $f'(x) = mx^{m-1}$. Then we show that $f(x) = x^{m+1}$ $f'(x) = (m+1)x^m$. 
We can factor $f(x) = x^{m+1}$ into $g(x) = x$, $h(x) = x^m$. 
Then by the product rule and inductive assumption that $h'(x) = mx^{m-1}$: $f' = f(x)g'(x) + f'(x)g(x) = (x)(mx^{m-1}) + (1)(x^m) = mx^m + x^m = (m+1)x^m$.

Since the base case and inductive step hold, the claim is true by mathematical induction.

\end{proof}

Then using the above relation it is easy to see that the derivative is continuous. 

\begin{proof}

Clearly it is continuous for any $x \neq 0$. So we have to show that it is continuous at $x=0$

We show that on both sides of zero the derivatives are equal. 
So on the left hand side ($x < 0$) $f_-(x) = 0$ which is a constant function so $f_-'(x) = 0$. 
Then by the above theorem on the right hand side ($x \geq 0$): $f_+(x) = x^k$ and $f_+'(x) = k x^{k-1}$. 
Since $k \geq 2$, $k - 1 > 0$. So at $x = 0$ $f_+'(0) = k \cdot 0^{k-1} = 0$. 

So the derivative of both the left and right hand sides are equal.  We can take the one sided limits of $\lim_{x \to 0 ^-} f_-'(x) = 0$ and $\lim_{x \to 0 ^+} f_+'(x) = 0$ so $\lim_{x \to 0} f(x)$ exists. So $f'(x)$ is continuous at 0

\end{proof}


\problem{2} 5.4.6 Problem 2

Suppose $f'(x_0) = 0, \sp f''(x_0) = 0, \sp ... \sp, f^{(n-1)}(x_0) = 0$ and $f^{(n)} (x_0) > 0$ for a $f \in C^n$. Prove that $f$ has a local minimum at $x_0$ if $n$ is even and that $x_0$ is neither a local max or local min if $n$ is odd. 

\textit{Hint: Use Taylor's Theorem 5.4.5}
\hrule 


\begin{proof}

Let $T_n$ denote the $n^{th}$ order Taylor approximation of $f$ around $x_0$. 
$T_n = f(x_0) + f'(x_0) (x - x_0) + f''(x_0) (x - x_0)^2 + ... + f^{(n)}(x_0) (x - x_0)^n$. We know that $T_n$ approximates $f$ around $x_0$ and the first $n$ derivatives of $T_n$ match those of $f$. 
So if there is a min or max of $f$ at $x_0$, $T$ will also have a min or max at $x_0$. We use the fact that $f^{(k)} = 0$ for $k = 1, 2, ... n-1$. 
Then we have $T_n(x) = f(x_0) + \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n$. 
If $n$ is even, then $(x - x_0)^n > 0 \sp \forall x \neq x_0$. 
Then we have it symmetric around $x_0$: $T_n(x_0 + 1/n) = T_n(x_0 - 1/n)$, and $T_n(x_1) > T_n(x_2) \sp for \sp |x_1 - x_0| > |x_2 - x_0|$. 
Then we have $T_n$ strictly decreasing from both the left and right hand side, so it has a local min. Thus $f$ also has a local min.

If $n$ is odd, then $(x - x_0)^n$ is not necessarily positive. We have $T_n$ decreasing from the right hand side, but increasing from the left hand side. So it is neither a local min or local max.


\end{proof}

\end{document}